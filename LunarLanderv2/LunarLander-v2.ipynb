{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be74066",
   "metadata": {},
   "source": [
    "# Lunar Lander Study\n",
    "### This notebook is a study of the LunarLander-v2 environment using the Gymnasium environment\n",
    "### The algorithm library is StableBaselines3\n",
    "\n",
    "*This notebook was created in Jupyter Notebooks and is based off of the HuggingFace Unit 1 tutorial*\n",
    "\n",
    "HuggingFace Tutorial: [unit1](https://github.com/huggingface/deep-rl-class/tree/main/unit1)\n",
    "\n",
    "Environment: [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
    "\n",
    "RL-Library: [StableBaselines3](https://stable-baselines3.readthedocs.io/en/master/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10582980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import (DummyVecEnv, VecMonitor)\n",
    "# utils is a python file containing useful scripts such as an mp4 video generator\n",
    "import video_save_utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a3981",
   "metadata": {},
   "source": [
    "### LunarLander-v2:\n",
    "It is important to have an understanding about both the observation and action space.\n",
    "The observation space will include all relevant data about the lander and the action space include all of the possible actions our agent can take. \n",
    "\n",
    "In this case, the observation space of the model includes the following:\n",
    "- Horizontal x coordinate of the lander\n",
    "- Horizontal y coordinate of the lander\n",
    "- Linear x velocity\n",
    "- Linear y velocity\n",
    "- The lander's angle\n",
    "- The lander's angular velocity\n",
    "- Left left ground contact boolean\n",
    "- Tight leg ground contact boolean\n",
    "\n",
    "The action space includes the following:\n",
    "- Do nothing\n",
    "- Fire left engine\n",
    "- Fire main engine\n",
    "- Fire right engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e89af3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gym environment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "# reset the gym environment\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d9c84f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:\n",
      "Shape: (8,)\n",
      "Sample: [-0.9326524   0.44668216 -0.16413926  0.1322865  -0.72383285  0.71300477\n",
      " -1.257994    0.55769825]\n"
     ]
    }
   ],
   "source": [
    "print(\"Observation Space:\")\n",
    "# prints out the shape of our observation space\n",
    "print(\"Shape: {}\".format(observation.shape))\n",
    "# prints out a random sample from our observation space\n",
    "print(\"Sample: {}\".format(env.observation_space.sample()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337f4239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space:\n",
      "Shape: 4\n",
      "Sample: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space:\")\n",
    "# prints out the shape of our observation space\n",
    "print(\"Shape: {}\".format(env.action_space.n))\n",
    "# prints out a random sample from our observation space\n",
    "print(\"Sample: {}\".format(env.action_space.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d6ba57",
   "metadata": {},
   "source": [
    "---\n",
    "For our training we want to use a vectorized envirnoment so that we can have more diverse experiences during training. This method runs multiple copies of the same environment in parallel and provides a linear speedup in steps taken through sampling the multiple sub-environments at the same time. ([Gymnasium Vectors](https://gymnasium.farama.org/api/vector/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc5ef30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v_env = make_vec_env('LunarLander-v2', n_envs = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a55a3f",
   "metadata": {},
   "source": [
    "Now if we print out a sample of the obsevation space we have a list of vectors or size 16x8 instead of a single observation vector.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2d61b",
   "metadata": {},
   "source": [
    "### PPO: Proximal Policy Optimization\n",
    "\n",
    "PPO: Combines the ideas of A2C (multiple workers) and TRPO (it uses a trust region to improve the actor) ([sb3](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D)). The main idea is that after and update, the new policy should not be too far from the old policy. According to the developers, this policy alternates between sampling data through interaction with the environment and optimizing a \"surrogate\" objective funtion using stochastic gradient ascent ([arxiv.org](https://arxiv.org/abs/1707.06347))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fd5a3",
   "metadata": {},
   "source": [
    "##### Hyperparameters:\n",
    "\n",
    "Reinforcement Learning is highly dependent on hyperparameters. In the case of the PPO we have several that we can tune and change. In our case, our inputs are a vector instead of a frame of the game so we should use an MlpPolicy. This gym example has actually been optimized already by RLZoo and we can use their parameters as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d40c1391",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = 'MlpPolicy'\n",
    "# learning rate\n",
    "lr = 0.0003\n",
    "# number of steps (state-action pairs) per environment update (epoch)\n",
    "n_steps = 2048 #changed from 1024\n",
    "# minibatch size, small ~8 medium ~64 large ~512\n",
    "batch_size = 128 #changed from 64\n",
    "# number of epochs when optimizing surrogate loss\n",
    "n_epochs = 8 #changed from 4\n",
    "# Discount factor\n",
    "gamma = 0.999\n",
    "# Factor for tradeoff of bias vs variance for Generalized Advantage Estimator\n",
    "gae_lambda = 0.98\n",
    "# Entropy coefficient\n",
    "ent_coef = 0.01\n",
    "# number of timesteps to train the agent\n",
    "n_timesteps = 1000000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0b1dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# define a model using our above hyperparameters\n",
    "model = PPO(policy = mlp,\n",
    "            env = v_env,\n",
    "            learning_rate = lr,\n",
    "            n_steps = n_steps,\n",
    "            batch_size = batch_size,\n",
    "            n_epochs = n_epochs,\n",
    "            gamma = gamma,\n",
    "            gae_lambda = gae_lambda,\n",
    "            ent_coef = ent_coef,\n",
    "            verbose = 1\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07aed9b",
   "metadata": {},
   "source": [
    "Next we train the agent. This process can be time consuming. Let's train for 1 million steps similar to the recommened hyperparameters from RLZoo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d0ed71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.8     |\n",
      "|    ep_rew_mean     | -198     |\n",
      "| time/              |          |\n",
      "|    fps             | 3081     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 89.8         |\n",
      "|    ep_rew_mean          | -137         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1825         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073218136 |\n",
      "|    clip_fraction        | 0.0747       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.000856    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6e+03      |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.0071      |\n",
      "|    value_loss           | 4.36e+03     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 92.1       |\n",
      "|    ep_rew_mean          | -119       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1636       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 60         |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00795465 |\n",
      "|    clip_fraction        | 0.0658     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0.000249   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 539        |\n",
      "|    n_updates            | 16         |\n",
      "|    policy_gradient_loss | -0.0068    |\n",
      "|    value_loss           | 1.47e+03   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 93.9        |\n",
      "|    ep_rew_mean          | -92.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1598        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011260841 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | 0.000769    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 266         |\n",
      "|    n_updates            | 24          |\n",
      "|    policy_gradient_loss | -0.00828    |\n",
      "|    value_loss           | 818         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 93.1        |\n",
      "|    ep_rew_mean          | -79.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1574        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009064896 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.00581    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 229         |\n",
      "|    n_updates            | 32          |\n",
      "|    policy_gradient_loss | -0.00782    |\n",
      "|    value_loss           | 539         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 99           |\n",
      "|    ep_rew_mean          | -66.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1524         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0118917115 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.3         |\n",
      "|    explained_variance   | -0.00255     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 139          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00885     |\n",
      "|    value_loss           | 284          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 108         |\n",
      "|    ep_rew_mean          | -43.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1511        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 229376      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011363039 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | -0.011      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 130         |\n",
      "|    n_updates            | 48          |\n",
      "|    policy_gradient_loss | -0.00854    |\n",
      "|    value_loss           | 328         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 114         |\n",
      "|    ep_rew_mean          | -30.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1495        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010556367 |\n",
      "|    clip_fraction        | 0.0662      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | -0.000179   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 132         |\n",
      "|    n_updates            | 56          |\n",
      "|    policy_gradient_loss | -0.00733    |\n",
      "|    value_loss           | 329         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 152         |\n",
      "|    ep_rew_mean          | -20.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1437        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 294912      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007866228 |\n",
      "|    clip_fraction        | 0.0662      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | -6.79e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 217         |\n",
      "|    n_updates            | 64          |\n",
      "|    policy_gradient_loss | -0.00564    |\n",
      "|    value_loss           | 418         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 249         |\n",
      "|    ep_rew_mean          | -7.16       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1338        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 244         |\n",
      "|    total_timesteps      | 327680      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008854406 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | -0.000377   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 243         |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00352    |\n",
      "|    value_loss           | 526         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 375         |\n",
      "|    ep_rew_mean          | -12.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1247        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 289         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007760277 |\n",
      "|    clip_fraction        | 0.0606      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.000317    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 254         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    value_loss           | 549         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 351         |\n",
      "|    ep_rew_mean          | 1.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1184        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 332         |\n",
      "|    total_timesteps      | 393216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005995433 |\n",
      "|    clip_fraction        | 0.0268      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.000913    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 136         |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | -0.00234    |\n",
      "|    value_loss           | 501         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 487         |\n",
      "|    ep_rew_mean          | 16.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1131        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 376         |\n",
      "|    total_timesteps      | 425984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006644818 |\n",
      "|    clip_fraction        | 0.0344      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.402       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 158         |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.00288    |\n",
      "|    value_loss           | 306         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 578          |\n",
      "|    ep_rew_mean          | 24.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1087         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 421          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058872253 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.653        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 85.1         |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | -0.00311     |\n",
      "|    value_loss           | 187          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 672          |\n",
      "|    ep_rew_mean          | 40.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1037         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 473          |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066938945 |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.75         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.7         |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    value_loss           | 157          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 800          |\n",
      "|    ep_rew_mean          | 58.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1001         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 523          |\n",
      "|    total_timesteps      | 524288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056821266 |\n",
      "|    clip_fraction        | 0.0424       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.863        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.4         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    value_loss           | 92.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 867          |\n",
      "|    ep_rew_mean          | 68.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 977          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 570          |\n",
      "|    total_timesteps      | 557056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065979417 |\n",
      "|    clip_fraction        | 0.0437       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.21        |\n",
      "|    explained_variance   | 0.903        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.7         |\n",
      "|    n_updates            | 128          |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 64           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 869          |\n",
      "|    ep_rew_mean          | 68.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 954          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 618          |\n",
      "|    total_timesteps      | 589824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046820934 |\n",
      "|    clip_fraction        | 0.0455       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | 0.895        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.8         |\n",
      "|    n_updates            | 136          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    value_loss           | 96           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 892         |\n",
      "|    ep_rew_mean          | 83.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 937         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 663         |\n",
      "|    total_timesteps      | 622592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004175098 |\n",
      "|    clip_fraction        | 0.0365      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.921       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.94        |\n",
      "|    n_updates            | 144         |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    value_loss           | 72.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 929          |\n",
      "|    ep_rew_mean          | 94.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 921          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 710          |\n",
      "|    total_timesteps      | 655360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060338173 |\n",
      "|    clip_fraction        | 0.0536       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.1         |\n",
      "|    n_updates            | 152          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    value_loss           | 49.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 937         |\n",
      "|    ep_rew_mean          | 103         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 910         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 755         |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006470362 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.3        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    value_loss           | 30.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 949          |\n",
      "|    ep_rew_mean          | 109          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 897          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 803          |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076989066 |\n",
      "|    clip_fraction        | 0.0293       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0.958        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.8         |\n",
      "|    n_updates            | 168          |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 41.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 943         |\n",
      "|    ep_rew_mean          | 118         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 887         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 848         |\n",
      "|    total_timesteps      | 753664      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004562903 |\n",
      "|    clip_fraction        | 0.0353      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.89        |\n",
      "|    n_updates            | 176         |\n",
      "|    policy_gradient_loss | -0.00279    |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 965         |\n",
      "|    ep_rew_mean          | 130         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 893         |\n",
      "|    total_timesteps      | 786432      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008163519 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.35        |\n",
      "|    n_updates            | 184         |\n",
      "|    policy_gradient_loss | -0.00169    |\n",
      "|    value_loss           | 9.86        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 977          |\n",
      "|    ep_rew_mean          | 136          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 862          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 950          |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038626327 |\n",
      "|    clip_fraction        | 0.0166       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.95         |\n",
      "|    n_updates            | 192          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 9.36         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 985          |\n",
      "|    ep_rew_mean          | 147          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 767          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 1109         |\n",
      "|    total_timesteps      | 851968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043454226 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.04         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 10.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 992         |\n",
      "|    ep_rew_mean          | 147         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 690         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 1281        |\n",
      "|    total_timesteps      | 884736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003734577 |\n",
      "|    clip_fraction        | 0.0305      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.99        |\n",
      "|    n_updates            | 208         |\n",
      "|    policy_gradient_loss | -0.0021     |\n",
      "|    value_loss           | 6.48        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 993          |\n",
      "|    ep_rew_mean          | 146          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 638          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 1436         |\n",
      "|    total_timesteps      | 917504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044227587 |\n",
      "|    clip_fraction        | 0.0356       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.89         |\n",
      "|    n_updates            | 216          |\n",
      "|    policy_gradient_loss | -0.00108     |\n",
      "|    value_loss           | 3.3          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 986          |\n",
      "|    ep_rew_mean          | 144          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 594          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 1597         |\n",
      "|    total_timesteps      | 950272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029816853 |\n",
      "|    clip_fraction        | 0.0222       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.95         |\n",
      "|    n_updates            | 224          |\n",
      "|    policy_gradient_loss | -0.000727    |\n",
      "|    value_loss           | 6.39         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 986         |\n",
      "|    ep_rew_mean          | 145         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 558         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 1759        |\n",
      "|    total_timesteps      | 983040      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003335575 |\n",
      "|    clip_fraction        | 0.0255      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.995       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 232         |\n",
      "|    policy_gradient_loss | -0.000923   |\n",
      "|    value_loss           | 7.56        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 985         |\n",
      "|    ep_rew_mean          | 148         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 532         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 1907        |\n",
      "|    total_timesteps      | 1015808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007704644 |\n",
      "|    clip_fraction        | 0.0389      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.963      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.55        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    value_loss           | 2.02        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x28ba5ea7dc0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=n_timesteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf5c522",
   "metadata": {},
   "source": [
    "### Training function outputs:\n",
    "- Rollout:\n",
    "    - ep_len_mean: mean episode length\n",
    "    - ep_rew_mean: mean epsiodic training reward averaged over 100 episodes\n",
    "- Time:\n",
    "    - fps: number of frames per second including the time taken by gradient updates\n",
    "    - iterations: number of iterations (data collection + policy update for A2C/PPO)\n",
    "    - time_elapsed: time in seconds since beginning of training\n",
    "    - total_timesteps: total number of timesteps since beginning of training\n",
    "- Train:\n",
    "    - approx_kl: approximate mean KL divergence between old and new policy (for PPO). An estimation of how much change happened in the update\n",
    "    - clip_fraction: mean fraction of surrogate loss that was clipped (aboce clip range threshold)\n",
    "    - clip_range: current value of clipping factor for surrogate loss\n",
    "    - entropy_loss: mean value of entropy loss (negative of the average of policy entropy)\n",
    "    - explained_variance: fraction of the return variance explained by the value function (ev=0 => might as well have predicted 0, ev=1 => perfect prediction, ev<0 => worse than predicting 0\n",
    "    - learning_rate: current learning rate\n",
    "    - loss: current total loss\n",
    "    - n_updates: number of gradient updates so far\n",
    "    - policy_gradient_loss: current value of policy gradient loss (value does not have much meaning)\n",
    "    - value_loss: Current value for value function loss for on-policy algorithms, usually the error between value function and Monte-Carlo estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2a6d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('ppo-LunarLanderv2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e5721",
   "metadata": {},
   "source": [
    "After creating and saving the model it is very important that we evaluate the model and see the results of the training and determine how well our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac14736f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=258.19 +/- 17.311628341674805\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load('ppo-LunarLanderv2')\n",
    "# Create and evaluation environment\n",
    "dummy_env = DummyVecEnv([lambda: env])\n",
    "eval_env = VecMonitor(dummy_env)\n",
    "# mean reward, standard reward refer to the average reward per episode and the standard of that reward\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes = 10, deterministic = True)\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c798f5",
   "metadata": {},
   "source": [
    "This is a good result! The mean reward is above 200 which means that the lander has successfully landed on the moon. Below we will save an mp4 file so that we can visualized the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of the evaluation environment for the replay environment\n",
    "replay_env = eval_env\n",
    "#create a length of video in timesteps\n",
    "video_length = 2000\n",
    "#set model to be deterministic\n",
    "is_d = True\n",
    "\n",
    "video_save_utility.generate_replay(model, replay_env, video_length, is_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d42b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
